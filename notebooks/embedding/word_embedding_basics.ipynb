{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing with word embeddings: essential methods and operations\n",
    "\n",
    "| Author | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2025-09-23 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/haukelicht/advanced_text_analysis/blob/main/notebooks/embedding/word_embedding_basics.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to use `gensim` to compute with word vectors (e.g., word2vec).\n",
    "\n",
    "We'll cover how to\n",
    "\n",
    "- compute two words similarity\n",
    "- find the most similar words for a focal word\n",
    "- solve analogy tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red\">*IMPORTANT:*</span>\n",
    "This notebook contains exercises that are marked with the keyword pattern `# TODO:`\n",
    "In cells containing this keyword, you _first_ **add** the necessary code to be able to successfully run the cell.\n",
    "\n",
    "For example, the following code won't run without completing the TODO:\n",
    "\n",
    "```python\n",
    "for i in range(4):\n",
    "    # TODO: add logic here!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # shallow clone of current state of main branch \n",
    "    !git clone --branch main --single-branch --depth 1 --filter=blob:none https://github.com/haukelicht/advanced_text_analysis.git\n",
    "    \n",
    "    !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load modules we'll need throughout the notebook here.\n",
    "\n",
    "<!-- **_Note:_** I asume you have follewed the setup instructions at https://github.com/haukelicht/advanced_text_analysis/tree/main/setup -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file in- and export\n",
    "import os\n",
    "\n",
    "# for working with word embeddings\n",
    "import gensim\n",
    "\n",
    "# for using arrays and data frames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a pre-trained word2vec model\n",
    "\n",
    "In this notebook, we will use a word2vec model that has been trained on the *Google News Corpus*.\n",
    "\n",
    "It is downloadable from the internet and we use the `gensim` downloader API to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained word2vec model (shipped with gensim)\n",
    "import gensim.downloader as api\n",
    "\n",
    "# load the model and name it's instance in our notebook environment 'word2vec'\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** The above code will download the model (if not already done so) to the folder specified in `api.BASE_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show path\n",
    "print(api.BASE_DIR)\n",
    "\n",
    "# list files in folder\n",
    "os.listdir(api.BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could load an already downloaded model from your computer:\n",
    "\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_name = 'word2vec-google-news-300'\n",
    "model_dir = os.path.join(api.BASE_DIR, model_name)\n",
    "model_path = os.path.join(model_dir, model_name + '.gz') \n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understanding the `word2vec` object\n",
    "\n",
    "Let's inspect the **`word2vec`** object we have created so you understand how it's structured, and what **attributes** and **methods** it has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding model is an instance of the `gensim.models.keyedvectors.KeyedVectors'` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the documentation with `?gensim.models.keyedvectors.KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all attributes\n",
    "list(model.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most relevant attributes are\n",
    "\n",
    "- `vector_size`: number of dimensions of embeddings \n",
    "- `index_to_key`: pyhton list listing words' indexes in the embedding matrix\n",
    "- `key_to_index`: pyhton list listing words in the embedding matrix\n",
    "- `vectors`: 2-dimensional `numpy` array recording word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a vocabulary-index-to-word mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(enumerate(model.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop at the `vectors` that store the word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the embedding matrix (number of words x number of embedding dimensions)\n",
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many storage/RAM needed for this matrix?\n",
    "n_bytes = model.vectors.nbytes\n",
    "gb = n_bytes / (1024**3)\n",
    "print(round(gb, 2), \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Essential methods available for `KeyedVectors` objects\n",
    "\n",
    "`gensim`'s  `KeyedVectors` class comes with a lot of useful pre-defined methods.\n",
    "\n",
    "Let's start with different ways of accessing a word's embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Accessing a word's embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: directly index the object with a string value\n",
    "print(model['good'][:10]) # <== look only at first 10 dimensions\n",
    "len(model['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: use the get_vector() method\n",
    "model.get_vector('good')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Finding where a word's embedding is located\n",
    "\n",
    "`model.key_to_index` maps the \"words\" (tokens) in the model's vocabulary to their index position in the emebdding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So an alternative way to access a word's embedding is \n",
    "\n",
    "1. using the model's `key_to_index` attribute to lookup a word's index in the embedding matrix, and\n",
    "2. index the model's embedding matrix accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = model.key_to_index['good']\n",
    "print(idx)\n",
    "model.vectors[idx][:10] # <== just print first 10 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualizing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since word embeddings have too many dimensions to display their locations, we usually apply dimensionality reduction techniques.\n",
    "\n",
    "In this notebook, we'll simply rely on *Principal Component Analysis* (PCA).\n",
    "Belo is some code that illustrates this.\n",
    "\n",
    "**_Note:_** There are better options like UMAP or t-SNE that do a better job in preserving relative distances between data points when reducing dimensionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    # beverages\n",
    "    'coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
    "    # animals\n",
    "    'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard', 'frog', 'monkey', 'ape', 'kangaroo', 'wolf',\n",
    "    # countries\n",
    "    'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
    "    # institutions\n",
    "    'parliament', 'government', 'opposition', 'coalition', 'prime_minister', 'minister',\n",
    "]\n",
    "\n",
    "X = np.array([model[w] for w in words if w in model.key_to_index])\n",
    "\n",
    "pcs = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "plt.scatter(pcs[:,0], pcs[:,1], c='r', edgecolors='k', alpha=.4)\n",
    "for word, (x, y) in zip(words, pcs):\n",
    "    plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Essential computations\n",
    "\n",
    "In the next subsections, we will learn how use `gensim`'s `KeyedVectors` objects to \n",
    "\n",
    "1. compute similarities, \n",
    "2. find nearest neighbors, and \n",
    "3. solve analogy tasks.\n",
    "\n",
    "**_Note:_** \n",
    "We will only go through the methods that will be required to complete exercises in our course.\n",
    "Visit Radim ≈òeh≈Ø≈ôek's [summary](https://radimrehurek.com/gensim/models/keyedvectors.html) for a more comprehensive overview of available methods with excellent examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Computing similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An central quantity of interest in the study of lexical semantics (i.e., the \"meaning\" of words) is **word similarity** &mdash; how similar the \"meanings\" of two words are.\n",
    "\n",
    "In CSS studies using word embeddings, computating similarities is often an essential computation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "\n",
    "In the embedding space, we can **measure two word vectors' similarity** of by their **_angle_**: the extent to which they *point in similar directions* relative to the coordiante origin $(0, 0, \\ldots, 0)$.\n",
    "\n",
    "Say you stand at the origin of a $d$-dimensional coordinate system.\n",
    "Then asking whether two vectors $\\mathbf{u}$ and $\\mathbf{v}$ are similar is the same as asking: \n",
    "\n",
    "> \"Would I end up in a similar location if I follow the direction of vector $\\mathbf{u}$ rather than $\\mathbf{v}$?\"\n",
    "\n",
    "The answer to this question aligns with the magnitude of the angle:\n",
    "\n",
    "- If the two vectors are very similar, you'd end up in a similar location (e.g., $\\mathbf{u} = [1, 1]$ and $\\mathbf{u} = [.8, .9]$)\n",
    "- If they are pointing in completely opposite directions, you'll end up on the other \"side\" of the vector space if you follow $\\mathbf{u}$ rather than $\\mathbf{v}$ (e.g., (e.g., $\\mathbf{u} = [1, 1]$ and $\\mathbf{u} = [-1, -1]$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration\n",
    "\n",
    "The below plot visually illustrates this, given the following vectors (i.e., 2D points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [\n",
    "    [ 1.0,  1.0], # vector a\n",
    "    [ 0.8,  0.9], # vector b\n",
    "    [ 0.5,  .2],  # vector c\n",
    "    [-1.0,  1.0], # vector d\n",
    "    [-1.0, -1.0], # vector e\n",
    "]\n",
    "vecs = np.array(vecs)\n",
    "vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot vectors' values on 1st and 2nd dimension on x and y axes\n",
    "plt.scatter(vecs[:,0], vecs[:,1])\n",
    "\n",
    "# add vertical and horizontal coordinate lines running through (0, 0) to the plot\n",
    "plt.axvline(0, c='grey', alpha=.5)\n",
    "plt.axhline(0, c='grey', alpha=.5)\n",
    "\n",
    "# draw lines from the coordinate origin (0, 0) to each point\n",
    "vec_names = ['a', 'b', 'c', 'd', 'e']\n",
    "for i, vec in enumerate(vecs):\n",
    "    plt.annotate(\n",
    "        '', # no text\n",
    "        xy=vec, # end point of the line\n",
    "        xytext=(0, 0), # start point of the line\n",
    "        arrowprops=dict(facecolor='black', color='black', width=.5, headwidth=3) # arrow style\n",
    "    )\n",
    "    plt.text(vec[0], vec[1], f'{vec_names[i]}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this example,\n",
    "\n",
    "- $\\mathbf{a}$ and $\\mathbf{b}$ are very similar (because they point in a similar direction)\n",
    "- $\\mathbf{a}$ and $\\mathbf{c}$, and $\\mathbf{b}$ and $\\mathbf{c}$ are also still somewhat similar\n",
    "- $\\mathbf{a}$ and $\\mathbf{d}$ are orthorgonal (\"unrelated\"), and\n",
    "- $\\mathbf{a}$ and $\\mathbf{e}$ are maximally dissimilar (because they point in opposite directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math\n",
    "\n",
    "The extent to which they point in similar directions relative to the coordiante origin $(0, 0,\n",
    " \\ldots, 0)$ can be measured by the **cosine similarity**:\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{{\\mathbf{u} \\cdot \\mathbf{v}}}{{\\| \\mathbf{u} \\| \\times \\| \\mathbf{v} \\|}}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\mathbf{u}$ and $\\mathbf{v}$ are the vectors whose similarity is being measured.\n",
    "\n",
    "- the *numerator* (top) measures the similarity of \n",
    "- the *denominator* (bottom) normalizes (\"rescales\") the similarity measure to range between -1 and 1 \n",
    "\n",
    "The $\\boldsymbol{\\cdot}$ operator denotes the **dot product** of two vectors. \n",
    "It measures the *similarity in the direction of two vectors.*\n",
    "(Think of vectors as directions from the origin $(0, 0, \\ldots, 0)$).\n",
    "\n",
    "- The dot product is computed as the sum of the element-wise products of vectors' entries: \n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = \\sum\\limits_{i=1}^{d} u_i \\times v_i\n",
    "$$\n",
    "- $u_i \\times v_i$ is high if the two entries are similar (point in the same directio nrelative to the coordinate origin $0$)\n",
    "- Thus, $\\mathbf{u} \\cdot \\mathbf{v}$ is the higher, the more of their $(u_i, v_i)$ elements are similar.\n",
    "\n",
    "$\\| \\mathbf{u} \\|$ and $\\| \\mathbf{v} \\|$ represent the **Euclidean norms** (magnitudes) of vectors $\\mathbf{u}$ and $\\mathbf{v}$.\n",
    "The magnitude (or length) of a vector measures its *distance from the origin in the  $d$-dimensional space.*\n",
    "It is computed as \n",
    "\n",
    "$$\n",
    "\\| \\mathbf{x} \\| = \\sqrt{ \\sum\\limits_{i=1}^{d} x_i^2}\n",
    "$$\n",
    "\n",
    "Having the $\\| \\mathbf{u} \\| \\times \\| \\mathbf{v} \\|$ in the denominator if the cosine similarity function makes the metric scale invariant (i.e., insensitive to the lengths/magnitudes of $\\mathbf{u}$ and $\\mathbf{v}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute the cosine similarity between two word vectors\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "    \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)\n",
    "        v -- a word vector of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute the dot product between u and v\n",
    "    dot = np.dot(u, v)\n",
    "    \n",
    "    # compute the Euclidean norm of u\n",
    "    norm_u = np.sqrt(np.sum(np.square(u)))\n",
    "    # compute the Euclidean norm of v\n",
    "    norm_v = np.sqrt(np.sum(np.square(v)))\n",
    "    \n",
    "    # divide the dot product by the product of the L2 norms\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** We implement our own `cosine_similarity` function here to map code to math. Below and in other notebooks, we'll use the `cosine` function from the `scipy.spatial.distance` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute similarties for vectors $\\mathbf{b}$-$\\mathbf{e}$ in `vecs` to vector $\\mathbf{a}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, vecs.shape[0]):\n",
    "    s = cosine_similarity(vecs[0], vecs[i])\n",
    "    print(f\"a = {vecs[0]}, {vec_names[i]} = {vecs[i]}, cos(a, {vec_names[i]}) = {s:+0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** inside `print` above we use python's string formatting syntax to directly insert the variable values we want to print.\n",
    "It works like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the same function to compute the similarity of two words in our `word2vec` embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute cosine similarity for two word vectors in our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(model['good'], model['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** `scipy` implements the cosine distiance metric for us ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "1-cosine(model['good'], model['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code (using `gensim``)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our `word2vec` instance of `gensim`'s `KeyedVectors` class also comes with a built-in method to compute two words' similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cosine similarity between two word embeddings in the word2vec object\n",
    "model.similarity('good', 'bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 üë©‚Äçüíª\n",
    "\n",
    "Let's implement a classic approach to evaluate how word embeddings capture cultural biases in their training copora.\n",
    "Here, we'll focus on **_gender bias_** &mdash; the differential association of traits and attributes with women and men (my lose definition).\n",
    "\n",
    "Compile a list `comparison_words` with occupations, character traits, and other words that might exhibit gender bias.\n",
    "Then compute how similar each word is with terms like 'man' and 'women', that indicate the male and female genders.\n",
    "\n",
    "Which words exhibit gender bias?\n",
    "And in which direction? \n",
    "Do you spot a pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_words = [\n",
    "    'programmer',\n",
    "    'scientist',\n",
    "    'smart',\n",
    "    'emotional',\n",
    "    'caring',\n",
    "    # TODO: add more interesting words here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarities to male and female terms\n",
    "male_terms = ['man']\n",
    "female_terms = ['woman']\n",
    "\n",
    "for word in comparison_words:\n",
    "    # TODO: implement logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: summarize the results in a table or figure\n",
    "# hint: if you have more than one term per gender, you might want to compute the average of comparison term--gender word similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Finding most similar terms (nearest neighbors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since word vectors capture similarities in words co-occurence patterns, their locations in the embedding space can be use to find lists of semantically and functionally similar words.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "In the embedding space, searching for similar words for a focus word *w* can be implemented as [*nearest neighbors* search](https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm) because words pairs' distances measure their similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration\n",
    "\n",
    "Let's use the same 2-dimensional vectors as in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [\n",
    "    [ 1.0,  1.0], # vector a\n",
    "    [ 0.8,  0.9], # vector b\n",
    "    [ 0.5,  .2],  # vector c\n",
    "    [-1.0,  1.0], # vector d\n",
    "    [-1.0, -1.0], # vector e\n",
    "]\n",
    "vecs = np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot vectors' values on 1st and 2nd dimension on x and y axes\n",
    "plt.scatter(vecs[:,0], vecs[:,1])\n",
    "\n",
    "# draw lines from the coordinates of vector a to vectors b, c, d, and e\n",
    "vec_names = ['b', 'c', 'd', 'e']\n",
    "for i, vec in enumerate(vecs[1:]):\n",
    "    plt.annotate(\n",
    "        '', # no text\n",
    "        xy=vec, # end point of the line\n",
    "        xytext=vecs[0], # start point of the line\n",
    "        arrowprops=dict(facecolor='black', color='black', width=.5, headwidth=3) # arrow style\n",
    "    )\n",
    "    plt.text(vec[0], vec[1], f'{vec_names[i]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "\n",
    "So we can find out what's the most similar vector to $\\mathbf{a}$ by \n",
    "\n",
    "1. computing all vectors ($\\mathbf{b}$-$\\mathbf{e}$) similarity with $\\mathbf{a}$, and \n",
    "2. selecting the one with the highest similarity among them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = {nm: cosine_similarity(vecs[0], vec) for nm, vec in zip(vec_names, vecs[1:])}\n",
    "print(sims)\n",
    "\n",
    "# get name of most similar values (by sorting)\n",
    "print([nm for nm, s in sorted(sims.items(), key=lambda item: item[1], reverse=True)][0])\n",
    "\n",
    "# same with numpy\n",
    "vec_names[np.array(sims.values).argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `gensim`'s `KeyedVectors` class provides dedicated methods for nearest neighbors-based similar word search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the five most similar words to 'love'\n",
    "model.most_similar('mother', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w, _ in model.most_similar('love', topn=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** The `most_similar()` method returns a list of tuples.\n",
    "Each tuple is a pair of word and cosine similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, among the five terms most similar to 'love', there is also it's [*antonym*](https://en.wikipedia.org/wiki/Opposite_(semantics)) 'hate' (i.e., opposite word).\n",
    "\n",
    "This illustrates that \"similar\" in the context of word embeddings does *not* necessarily mean [*synonym*](https://en.wikipedia.org/wiki/Synonym) (i.e.. equal in meaning).\n",
    "Instead, word embeddings capture a wide range of lexical semantic relations\n",
    "\n",
    "- synonyms\n",
    "- antonyms\n",
    "- hypo- and hypernym relations (subtyoe relations): https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy\n",
    "- mero- and hoponym relations (part-of relations): https://en.wikipedia.org/wiki/Meronymy_and_holonymy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 üë©‚Äçüíª\n",
    "\n",
    "Let's use nearest neighbors search to find conceptually equivalent terms for a \"seed\" word.\n",
    "\n",
    "**_Note:_** This is a typical task in expanding keyword lists for dictionaries.\n",
    "\n",
    "You can choose which seed word you want to start with (see example below for a suggestion).\n",
    "But while going through nearest neighbors, keep track of how many of the candidate terms in the top-20 or top-50 terms (or so) you would inlcude in your dictionary, and how many you would discard!\n",
    "\n",
    "**_Example_**: \n",
    "Say you want to compile a dictionary that contains typical words used to express *positive emotions*.\n",
    "In this case, you could start with the seed word 'happy.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w, s in model.most_similar('happy', topn=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: I made decisions\n",
    "'glad'  # approved\n",
    "'pleased', # approved\n",
    "'ecstatic', # approved\n",
    "'overjoyed', # approved\n",
    "'thrilled', # approved\n",
    "'satisfied', # approved\n",
    "'proud', # approved\n",
    "'delighted', # approved\n",
    "'disappointed', # not approved <== !!!\n",
    "'excited', # approved\n",
    "'happier', # approved\n",
    "'Said_Hirschbeck', # not approved <== !!!\n",
    "# ... and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_word = # TODO: pick another emotion word, list the 20 most similar words, and decide which ones you would approve as well\n",
    "[w for w, s in model.most_similar(emo_word, topn=20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Computing centroids\n",
    "\n",
    "Just as we use keywords in dictionaries to indicate concepts (e.g., positive affect), \n",
    "we can use bags of word vectors to \"locate\" concepts in the embedding space.\n",
    "\n",
    "Because each indicator word's embedding might caputure a concept only partially, researchers often **average multiple words' embeddings into _centroids_**.\n",
    "\n",
    "This is as simple as computing the mean of vectors value for each embedding dimension.\n",
    "And since word vectors in `gensim` are just numpy arrays, it takes no more than two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take example of words used to indicate concept 'positive sentiment'\n",
    "positive_sentiment_words = [\n",
    "    'glad',\n",
    "    'pleased',\n",
    "    'ecstatic',\n",
    "    'overjoyed',\n",
    "    'thrilled',\n",
    "    'satisfied',\n",
    "    'proud',\n",
    "    'delighted',\n",
    "    'excited',\n",
    "    'elated',\n",
    "    'thankful',\n",
    "    'enthused',\n",
    "    'chuffed',\n",
    "    'grateful',\n",
    "    'confident',\n",
    "]\n",
    "\n",
    "# get word vectors for words in list\n",
    "vecs = [model[w] for w in positive_sentiment_words if w in model.key_to_index]\n",
    "len(vecs) > 0 # <== always ensure that this has more than 0 elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.array(vecs)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average them dimension-wise\n",
    "centroid = np.average(vecs, axis=0)\n",
    "\n",
    "# this results in a single 300-dimensional vector\n",
    "centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is again a pre-implemented method\n",
    "centroid = model.get_mean_vector(keys=positive_sentiment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: also useful to compute \"sentence embedding\"\n",
    "sentence = \"Just write a random sentence.\"\n",
    "words = sentence.split(' ')\n",
    "words = [w.replace('.', '').lower() for w in words ]\n",
    "sentence_embedding = model.get_mean_vector(keys=words) # <== you should at least re-weight by words tf-idf values!\n",
    "# but see: https://openreview.net/pdf?id=SyK00v5xx (but the paper is suuuuuuuper old!)\n",
    "sentence_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    if word not in model:\n",
    "        print(word, ': -/-')\n",
    "    else:\n",
    "        print(word, ':', cosine_similarity(sentence_embedding, model[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine original values with centroid\n",
    "vecs = np.concatenate([vecs, centroid[np.newaxis, :]])\n",
    "\n",
    "# reduce embeddings to 2 dimensions\n",
    "X = np.array(vecs)\n",
    "pcas = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# plot\n",
    "plt.scatter(pcas[:-1,0], pcas[:-1,1], c='black')\n",
    "plt.scatter(pcas[-1,0], pcas[-1, 1], c='red')\n",
    "plt.text(pcas[-1,0], pcas[-1, 1], 'centroid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excurse: Using centroids to finding mismatching terms (the \"odd one out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use centroids to find the most mismatching term in a lits of terms.\n",
    "\n",
    "For this, we \n",
    "\n",
    "1. compute the centroid of all  but the focus term's embeddings,\n",
    "2. compute the similarity of the focus term's embedding and the centroids,\n",
    "3. repeat this leave-one-out procedure for all terms, and\n",
    "4. label the one with the lowest to-centroid similarity as 'mismatching'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\n",
    "    'cat',\n",
    "    'dog',\n",
    "    'hamster',\n",
    "    'flower',\n",
    "    'table'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a principal component analysis to visualize\n",
    "X = np.array([model[w] for w in terms])\n",
    "pcs = PCA(n_components=2).fit_transform(X)\n",
    "plt.scatter(pcs[:,0], pcs[:,1])\n",
    "# label the points\n",
    "for i, (x, y) in enumerate(pcs):\n",
    "    plt.text(x, y, f'{terms[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one term\n",
    "c = np.average([model[w] for w in terms[:-1]], axis=0)\n",
    "terms[-1], cosine_similarity(c, model[terms[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all terms\n",
    "sims = list()\n",
    "for term in terms:\n",
    "    c = np.average([model[w] for w in terms if w != term], axis=0)\n",
    "    s = cosine_similarity(c, model[term])\n",
    "    sims.append(s)\n",
    "print(sims)\n",
    "\n",
    "# find index of lowest value\n",
    "odd_one_out = terms[np.argmin(sims)]\n",
    "print(f\"the mismatching term is: '{odd_one_out}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, `gensim`'s `KeyedVectors` class again provides dedicated methods for nearest neighbors-based similar word search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Solving analogies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Well-behaved\" word embeddings exhibit the ability to automate much more complex tasks than finding sysnonyms, antonyms, and mismatching terms.\n",
    "\n",
    "One of word2vec's powers that has attracted a lot of attention when Tomas Mikolov *et al.* released  it ten years ago was their ability to solve analogy tasks.\n",
    "\n",
    "**Analogy tasks** are simple national language inference problems that prompt a model to infer the Y2 in the following relational statement: \n",
    "\n",
    "> `X1 is to Y1 as X2 is to Y2.`\n",
    "\n",
    "For example, a frequently cited analogy problem is,\n",
    "\n",
    "> `Man is to woman as king is to _____?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "\n",
    "One can solve analogy problems with word embeddings by performing applying the following geometric operations:\n",
    "\n",
    "1. View the statement ` X1 is to Y1 ` as an instruction to move from the location of X1 in the direction of Y1.\n",
    "2. To get to Y2, take the same direction but start from X2.\n",
    "\n",
    "The following figure from [Jurafsky & Martin (2023)](https://web.stanford.edu/~jurafsky/slp3/6.pdf) illustrates this idea for the analogy \"apple is to tree as grape is to vine:\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/analogy_problem_jurafsky.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math\n",
    "\n",
    "Let's take the analogy problem\n",
    "\n",
    "> 'man' is to 'woman' as 'king' is to '_____'?\n",
    "\n",
    "This can be \"solved\" by computing with word vectors by performing\n",
    "the following arithmetic operation with the word vectors of, king, man, and woman:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_\\text{man} - \\mathbf{v}_\\text{woman} &\\approx \\mathbf{v}_\\text{king} - \\mathbf{v}_\\text{?}\\\\\n",
    "\n",
    "\\mathbf{v}_\\text{king} - \\mathbf{v}_\\text{man} + \\mathbf{v}_\\text{woman} &\\approx \\mathbf{v}_\\text{?}\n",
    "\\end{align}\n",
    "\n",
    "The **closest word vector to** $\\mathbf{v}_\\text{?}$ is the best model's best guess for completing the analogy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.key_to_index['bank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because\n",
    "\n",
    "- the operation $(\\mathbf{v}_\\text{woman} - \\mathbf{v}_\\text{man})$ spans a \"gender dimension\": $\\mathbf{v}_\\text{gender}$\n",
    "- adding $\\mathbf{v}_\\text{gender}$ to $\\mathbf{v}_\\text{king}$ means to depart from 'king' and move along the gender dimension into the direction of \"femininity\"\n",
    "\n",
    "(see Kozlowski et al., 2019, p. 912, for further examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_king = model['king']\n",
    "v_man = model['man']\n",
    "v_woman = model['woman']\n",
    "\n",
    "v_q = v_king - v_man + v_woman\n",
    "print(v_q.shape)\n",
    "\n",
    "model.similar_by_vector(v_q, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code (with `gensim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find the order in which you need to pass the words to the `positive` and `negative` arguments slightly unintuitive, though.\n",
    "So let's define a wrapper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(x1='man', y1='king', x2='woman', verbose=True):\n",
    "    \"\"\"Computes answer to query 'y1 is to x1 what WORD is to x2?'\"\"\"\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    if verbose:\n",
    "        print(f\"'{x1}' : '{y1}' :: '{x2}' : ?? ==> '{result[0][0]}'\")\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 üë©‚Äçüíª\n",
    "\n",
    "Can you come up with analogy problems involving terms from your discipline or research area?\n",
    "Can the word embedding model solve these specialized problems?\n",
    "\n",
    "**_Example:_** In politics \"Democrat is to progressive what Republican is to ___?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_x1 = model['Democrat']\n",
    "v_y1 = model['progressive']\n",
    "v_x2 = model['Republican']\n",
    "\n",
    "v_q = v_y1 - v_x1 + v_x2\n",
    "\n",
    "model.similar_by_vector(v_q, topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
