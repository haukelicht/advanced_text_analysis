{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025017f5",
   "metadata": {},
   "source": [
    "# Zero-shot text classification with LLMs\n",
    "\n",
    "This notebook illustrates how to use different LLMs for text classification.\n",
    "\n",
    "- closed-source LLMs models by OpenAI\n",
    "- open-weights model hosted via Hugging Face Inference Providers/Endpoints\n",
    "- open-weights LLMs models with `ollama`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5afba0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da313a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from src.utils.io import read_tabular\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f56a5d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False # no support for colab yet\n",
    "base_path = Path(\"/content/advanced_text_analysis/\" if COLAB else \"../../\")\n",
    "data_path = base_path / \"data\" / \"labeled\" / \"benoit_crowdsourced_2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (down)load the data\n",
    "fp = data_path / \"benoit_crowdsourced_2016-policy_area.csv\"\n",
    "if not fp.exists():\n",
    "    url = \"https://cta-text-datasets.s3.eu-central-1.amazonaws.com/labeled/\" + fp.parent.name + '/' + fp.name\n",
    "    df = pd.read_csv(url)\n",
    "    fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(fp, index=False)\n",
    "\n",
    "df = read_tabular(fp, columns=['text', 'label', 'metadata__gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5990bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to gold examples (i.e., those labeled by experts)\n",
    "df = df[df.metadata__gold]\n",
    "del df['metadata__gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8944ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    2: 'economic',\n",
    "    3: 'social',\n",
    "    1: 'neither',\n",
    "}\n",
    "df.label = df.label.map(id2label)\n",
    "\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get five examples per label class\n",
    "expls = df.groupby('label').sample(20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e11918",
   "metadata": {},
   "source": [
    "## Define the task\n",
    "\n",
    "In this example, we adapt the instruction for one of the tweet classification tasks examined in Benoit et al. ([2016](https://doi.org/10.1017/S0003055416000058)) \"Crowd-sourced Text Analysis: Reproducible and Agile Production\n",
    "of Political Data\"\n",
    "\n",
    "- see [this README file](../../data/labeled/benoit_crowdsourced_2016/README.md) for a description of the data and tasks covered in the paper\n",
    "- see [this file](../../data/labeled/benoit_crowdsourced_2016/instructions/econ_social_policy.md) for a copy of their original task instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f\"\"\"\n",
    "Act as a text classification system versatile in performing content analysis.\n",
    "\n",
    "You will read a sentence from a political text.\n",
    "Yout will judge whether this sentence deals with economic or social policy.\n",
    "You must classify posts into one of the following categories: \"economic\", \"social\", or \"neither\". \n",
    "\n",
    "## Definitions\n",
    "\n",
    "These categories have the following definitions:\n",
    "\n",
    "- Sentences should be coded as \"economic\" if they deal with aspects of the economy, such as: Taxation, Government spending, Services provided by the government or other public bodies, Pensions, unemployment and welfare benefits, and other state benefits, Property, investment and share ownership, public or private, Interest rates and exchange rates, Regulation of economic activity, public or private, Relations between employers, workers and trade unions\n",
    "- Sentences should be coded as \"social\" if they deal with aspects of social and moral life, relationships between social groups, and matters of national and social identity. These include: Policing, crime, punishment and rehabilitation of offenders; Immigration, relations between social groups, discrimination and multiculturalism; The role of the state in regulating the social and moral behavior of individuals\n",
    "\n",
    "## Step-by-step instructions\n",
    "\n",
    "Follow these steps to classify the sentence:\n",
    "\n",
    "1. Carefully read the text of the sentence, paying close attention to details.\n",
    "2. Assess whether the sentence belongs to any of the categories. If not, return 'neither' as your response.\n",
    "3. Classify the sentence with the category it belongs to. Return only the name of the category.\n",
    "\n",
    "## Response format\n",
    "\n",
    "Only include the selected category in your response and no further text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95844713",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = expls.text.to_list()\n",
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773717a",
   "metadata": {},
   "source": [
    "## With ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90464e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "MODEL = 'gpt-4o-2024-08-06'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d1d75",
   "metadata": {},
   "source": [
    "#### illustration with a _single_ sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e912bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596973d",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373b62c",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb21f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, model):\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    "  )\n",
    "  \n",
    "  result = response.choices[0].message.content\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6788aed",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_gpt4o = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_gpt4o,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106fc12",
   "metadata": {},
   "source": [
    "\n",
    "#### Caveate {style=\"color: orange\"}\n",
    "\n",
    "The annoying thing about OpenAI is that their models are closed-source, meaning we have no access to them.\n",
    "This limits reproducibility (see Palmer et al. [2024](https://www.nature.com/articles/s43588-023-00585-1)).\n",
    "\n",
    "So instead of relying them, we can use \"open-weights\" models.\n",
    "These are models for which we can freely download the model weights (i.e., paramters).\n",
    "We examine two options below: \n",
    "\n",
    "1. using Hugging Face _Inference Providers_ (via API)\n",
    "2. using Ollama (run locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd7288",
   "metadata": {},
   "source": [
    "## With Hugging Face _Inference Providers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de141bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "MODEL = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "client = InferenceClient(MODEL, token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc7e19",
   "metadata": {},
   "source": [
    "the **cool thing** is that the `InferenceClient` works exactly like the `openai.Client` class.\n",
    "So the code from above really _doesn't change_!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9161708",
   "metadata": {},
   "source": [
    "#### illustration with a _single_ sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227681fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9781c",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c385588",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, model):\n",
    "  # NOTE: `model` actually not needed because we setup the InferenceClient with the model already\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0.001,\n",
    "    seed=42\n",
    "  )\n",
    "  \n",
    "  result = response.choices[0].message.content\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c57f5a",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_llama3_70b = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47463ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_llama3_70b,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8cf96",
   "metadata": {},
   "source": [
    "Alright, the performance is slightly lower but this is only one of many available models.\n",
    "\n",
    "What if we try the very famous R1 model from DeepSeek?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8996dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL= \"deepseek-ai/DeepSeek-V3-0324\"\n",
    "\n",
    "client = InferenceClient(MODEL, provider=\"sambanova\", token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_text(text=df.text.iloc[5], system_message=instructions, model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_deepseekR1 = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_deepseekR1,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563c5c6",
   "metadata": {},
   "source": [
    "Well this didn't get any better but we could try other models very flexible, see [here](https://huggingface.co/inference/models) for available models by different providers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845ec56",
   "metadata": {},
   "source": [
    "## With Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e180bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client()\n",
    "MODEL = 'gemma3:4b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list models\n",
    "available_models = [m['model'] for m in client.list()['models']]\n",
    "\n",
    "if MODEL not in available_models:\n",
    "    import ollama\n",
    "    ollama.pull(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f385a9d",
   "metadata": {},
   "source": [
    "### Iterate over multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdacff",
   "metadata": {},
   "source": [
    "Let's first define a custom function to classify tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, system_message, model):\n",
    "\n",
    "  # clean the text \n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  # construct input\n",
    "\n",
    "  messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "\n",
    "  # set some options controlling generation behavior\n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  opts = {\n",
    "      'seed': 42,         # seed controlling random number generation and thus stochastic generation\n",
    "      'temperature': 0.0, # hyper parameter controlling \"craetivity\", see https://learnprompting.org/docs/basics/configuration_hyperparameters\n",
    "      'max_tokens': 3     # maximum numbers of tokens to generate in completion\n",
    "  }\n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  response = client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    options=opts\n",
    "  )\n",
    "  \n",
    "  # NOTE: this changed slightly compared to using `openai` Client\n",
    "  result = response.message.content.strip()\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first call, it migth take some tome because the model needs to be loaded first\n",
    "classify_text(texts[5], instructions, MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627de57f",
   "metadata": {},
   "source": [
    "Now we can iterate over example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_gemma3_4b = [\n",
    "    classify_text(text, instructions, model=MODEL)\n",
    "    for text in tqdm(texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69172cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(\n",
    "    y_true=expls.label,\n",
    "    y_pred=classifications_gemma3_4b,\n",
    ")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647dd2c0",
   "metadata": {},
   "source": [
    "## With `transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37404fbc",
   "metadata": {},
   "source": [
    "**_Caveat:_** We can only use a very small LLM for illustrative purposes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac366c3",
   "metadata": {},
   "source": [
    "Note: on CUDA GPU you can alos use quantization:\n",
    "\n",
    "```pyhton\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(..., quantization_config=bnb_config, ...)\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d58701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "\n",
    "# load the model and tokenizer\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, pad_token_id=tokenizer.eos_token_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.iloc[5]\n",
    "print(text)\n",
    "\n",
    "messages = [\n",
    "    # system prompt\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    # user input\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea12272",
   "metadata": {},
   "source": [
    "We first need to apply the chat template so we can perfrom [chat completion](https://huggingface.co/docs/inference-providers/en/tasks/chat-completion) instead of mere text generation/completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabaea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(chat_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66dc6e",
   "metadata": {},
   "source": [
    "As you see, this just converts the list of messages into text by adding special tokens that demarcate text by the assistant and user.\n",
    "\n",
    "Next, we need to tokenizer this input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(chat_messages, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device) # move to same device as model (GPU if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98988144",
   "metadata": {},
   "source": [
    "The tokenized inputs can be processed through the model to generate a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab176f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=3, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d243be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = inputs['input_ids'].shape[1]\n",
    "response = tokenizer.decode(outputs[0][offset:].cpu(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ae833",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8da61",
   "metadata": {},
   "source": [
    "## Inter-LLM agreement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9289c",
   "metadata": {},
   "source": [
    "What if we consider the different LLM's classifications as annotations?\n",
    "Then we compute see the degree of their inter-annotator agreement (ICA).\n",
    "\n",
    "This is equivalent to what we did in the [notebook](../annotation/compute_ica_pledge_classification.ipynb) on computing ICA in our policy pledge codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d27bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from krippendorff import alpha\n",
    "\n",
    "tmp = pd.DataFrame({\n",
    "    'gpt4o': classifications_gpt4o,\n",
    "    'gemma3_4b': classifications_gemma3_4b,\n",
    "    'llama3_70b': classifications_llama3_70b,\n",
    "    'deepseekR1': classifications_deepseekR1,\n",
    "})\n",
    "\n",
    "label2id = {\n",
    "    'economic': 0,\n",
    "    'social': 1,\n",
    "    'neither': 2,\n",
    "}\n",
    "\n",
    "tmp = tmp.apply(lambda x: x.map(label2id))\n",
    "alpha(tmp.T.values, level_of_measurement='nominal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcec6e",
   "metadata": {},
   "source": [
    "ðŸ˜³ This is a very strong agreement between LLMs' classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e20732",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
